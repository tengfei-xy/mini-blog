1. 设计目标：更细粒度的弹性扩缩容、高并发读写、数据不丢不错、多副本一致性以及高可用、支持分布式事务
2. TiKV单节点选择基于LSM-tree的RocksDB引擎。非常成熟、支持批量写入、无锁快照读（数据库副本迁移过程其作用）
3. 如何实现扩展？

使用Range分片方式，1.更高效扫描，2.实现自动分裂和合并更简单，3.弹性有限，分片可以自由调度。

数据存储、访问、复制、带哦度都是以**Region**为单位（默认96MB）。

全局有序map（Hash分区，Key和Value都是byte数组，使用Range分片）

4. Raft：确保数据能复制到足够多的副本
5. 事务：利用多版本控制（MVCC），从原来的keyN -> Value到keyN_Version1 -> Value
6. 分布式事物模式
   - 去中心化的两阶段提交
     - 通过PD全局授时（TSO）
     - ~4M timestamps每秒
     - 每个TiKV节点分配单独区域存放锁信息（CF lock）
   - Google Percolator事务模型
   - TiKV支持完整事务KV API
   - 默认支持乐观事务模型，也支持悲观事务模型（3.0+版本）
   - 默认隔离级别：Snapshot lsolation